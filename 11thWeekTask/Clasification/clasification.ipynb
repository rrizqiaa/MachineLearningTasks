{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
       "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
       "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
       "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
       "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   2     3       0  \n",
       "1   0     3       0  \n",
       "2   0     3       0  \n",
       "3   1     3       0  \n",
       "4   3     2       0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"heart.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Display missing values in the dataset\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((820, 13), (205, 13), (820,), (205,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = data.drop(columns=['target'])  # Features\n",
    "y = data['target']  # Target label\n",
    "\n",
    "# Standardize the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shapes of the resulting splits\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP with 1 hidden layer (16 neurons)...\n",
      "Epoch 1/20, Loss: 0.6907\n",
      "Epoch 2/20, Loss: 0.6006\n",
      "Epoch 3/20, Loss: 0.5343\n",
      "Epoch 4/20, Loss: 0.4777\n",
      "Epoch 5/20, Loss: 0.4341\n",
      "Epoch 6/20, Loss: 0.4029\n",
      "Epoch 7/20, Loss: 0.3838\n",
      "Epoch 8/20, Loss: 0.3696\n",
      "Epoch 9/20, Loss: 0.3605\n",
      "Epoch 10/20, Loss: 0.3521\n",
      "Epoch 11/20, Loss: 0.3456\n",
      "Epoch 12/20, Loss: 0.3375\n",
      "Epoch 13/20, Loss: 0.3341\n",
      "Epoch 14/20, Loss: 0.3283\n",
      "Epoch 15/20, Loss: 0.3263\n",
      "Epoch 16/20, Loss: 0.3201\n",
      "Epoch 17/20, Loss: 0.3195\n",
      "Epoch 18/20, Loss: 0.3132\n",
      "Epoch 19/20, Loss: 0.3132\n",
      "Epoch 20/20, Loss: 0.3097\n",
      "Evaluating MLP with 1 hidden layer (16 neurons)...\n",
      "Accuracy: 0.8537\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define a function to create an MLP model dynamically\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers):\n",
    "        super(MLPModel, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for neurons in hidden_layers:\n",
    "            layers.append(nn.Linear(current_size, neurons))\n",
    "            layers.append(nn.ReLU())  # Activation function\n",
    "            current_size = neurons\n",
    "        layers.append(nn.Linear(current_size, 2))  # Output layer (binary classification)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Experiment with 1 hidden layer (16 neurons)\n",
    "input_size = X_train.shape[1]\n",
    "hidden_layers = [16]\n",
    "\n",
    "mlp_model = MLPModel(input_size, hidden_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training MLP with 1 hidden layer (16 neurons)...\")\n",
    "train_model(mlp_model, train_loader, criterion, optimizer)\n",
    "print(\"Evaluating MLP with 1 hidden layer (16 neurons)...\")\n",
    "accuracy_1_hidden = evaluate_model(mlp_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP with 2 hidden layers (16, 32 neurons)...\n",
      "Epoch 1/20, Loss: 0.6722\n",
      "Epoch 2/20, Loss: 0.6259\n",
      "Epoch 3/20, Loss: 0.5509\n",
      "Epoch 4/20, Loss: 0.4560\n",
      "Epoch 5/20, Loss: 0.3760\n",
      "Epoch 6/20, Loss: 0.3363\n",
      "Epoch 7/20, Loss: 0.3166\n",
      "Epoch 8/20, Loss: 0.3063\n",
      "Epoch 9/20, Loss: 0.2944\n",
      "Epoch 10/20, Loss: 0.2865\n",
      "Epoch 11/20, Loss: 0.2817\n",
      "Epoch 12/20, Loss: 0.2739\n",
      "Epoch 13/20, Loss: 0.2679\n",
      "Epoch 14/20, Loss: 0.2634\n",
      "Epoch 15/20, Loss: 0.2571\n",
      "Epoch 16/20, Loss: 0.2520\n",
      "Epoch 17/20, Loss: 0.2472\n",
      "Epoch 18/20, Loss: 0.2433\n",
      "Epoch 19/20, Loss: 0.2335\n",
      "Epoch 20/20, Loss: 0.2313\n",
      "Evaluating MLP with 2 hidden layers (16, 32 neurons)...\n",
      "Accuracy: 0.8732\n",
      "\n",
      "Training MLP with 3 hidden layers (16, 32, 64 neurons)...\n",
      "Epoch 1/20, Loss: 0.6512\n",
      "Epoch 2/20, Loss: 0.5154\n",
      "Epoch 3/20, Loss: 0.3914\n",
      "Epoch 4/20, Loss: 0.3629\n",
      "Epoch 5/20, Loss: 0.3339\n",
      "Epoch 6/20, Loss: 0.3229\n",
      "Epoch 7/20, Loss: 0.3091\n",
      "Epoch 8/20, Loss: 0.2937\n",
      "Epoch 9/20, Loss: 0.2771\n",
      "Epoch 10/20, Loss: 0.2678\n",
      "Epoch 11/20, Loss: 0.2496\n",
      "Epoch 12/20, Loss: 0.2377\n",
      "Epoch 13/20, Loss: 0.2226\n",
      "Epoch 14/20, Loss: 0.2123\n",
      "Epoch 15/20, Loss: 0.1940\n",
      "Epoch 16/20, Loss: 0.1816\n",
      "Epoch 17/20, Loss: 0.1743\n",
      "Epoch 18/20, Loss: 0.1637\n",
      "Epoch 19/20, Loss: 0.1475\n",
      "Epoch 20/20, Loss: 0.1325\n",
      "Evaluating MLP with 3 hidden layers (16, 32, 64 neurons)...\n",
      "Accuracy: 0.9512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8536585365853658, 0.8731707317073171, 0.9512195121951219)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with 2 hidden layers (16, 32 neurons)\n",
    "hidden_layers_2 = [16, 32]\n",
    "\n",
    "mlp_model_2 = MLPModel(input_size, hidden_layers_2)\n",
    "optimizer_2 = optim.Adam(mlp_model_2.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining MLP with 2 hidden layers (16, 32 neurons)...\")\n",
    "train_model(mlp_model_2, train_loader, criterion, optimizer_2)\n",
    "print(\"Evaluating MLP with 2 hidden layers (16, 32 neurons)...\")\n",
    "accuracy_2_hidden = evaluate_model(mlp_model_2, test_loader)\n",
    "\n",
    "# Experiment with 3 hidden layers (16, 32, 64 neurons)\n",
    "hidden_layers_3 = [16, 32, 64]\n",
    "\n",
    "mlp_model_3 = MLPModel(input_size, hidden_layers_3)\n",
    "optimizer_3 = optim.Adam(mlp_model_3.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining MLP with 3 hidden layers (16, 32, 64 neurons)...\")\n",
    "train_model(mlp_model_3, train_loader, criterion, optimizer_3)\n",
    "print(\"Evaluating MLP with 3 hidden layers (16, 32, 64 neurons)...\")\n",
    "accuracy_3_hidden = evaluate_model(mlp_model_3, test_loader)\n",
    "\n",
    "# Compare accuracies\n",
    "accuracy_1_hidden, accuracy_2_hidden, accuracy_3_hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP with Linear activation function...\n",
      "Epoch 1/20, Loss: 0.5665\n",
      "Epoch 2/20, Loss: 0.4077\n",
      "Epoch 3/20, Loss: 0.3673\n",
      "Epoch 4/20, Loss: 0.3619\n",
      "Epoch 5/20, Loss: 0.3572\n",
      "Epoch 6/20, Loss: 0.3618\n",
      "Epoch 7/20, Loss: 0.3630\n",
      "Epoch 8/20, Loss: 0.3581\n",
      "Epoch 9/20, Loss: 0.3568\n",
      "Epoch 10/20, Loss: 0.3573\n",
      "Epoch 11/20, Loss: 0.3585\n",
      "Epoch 12/20, Loss: 0.3578\n",
      "Epoch 13/20, Loss: 0.3565\n",
      "Epoch 14/20, Loss: 0.3584\n",
      "Epoch 15/20, Loss: 0.3577\n",
      "Epoch 16/20, Loss: 0.3575\n",
      "Epoch 17/20, Loss: 0.3539\n",
      "Epoch 18/20, Loss: 0.3580\n",
      "Epoch 19/20, Loss: 0.3584\n",
      "Epoch 20/20, Loss: 0.3555\n",
      "Evaluating MLP with Linear activation function...\n",
      "Accuracy: 0.8098\n",
      "\n",
      "Training MLP with Sigmoid activation function...\n",
      "Epoch 1/20, Loss: 0.6969\n",
      "Epoch 2/20, Loss: 0.6915\n",
      "Epoch 3/20, Loss: 0.6903\n",
      "Epoch 4/20, Loss: 0.6852\n",
      "Epoch 5/20, Loss: 0.6724\n",
      "Epoch 6/20, Loss: 0.6477\n",
      "Epoch 7/20, Loss: 0.5884\n",
      "Epoch 8/20, Loss: 0.5022\n",
      "Epoch 9/20, Loss: 0.4269\n",
      "Epoch 10/20, Loss: 0.3887\n",
      "Epoch 11/20, Loss: 0.3746\n",
      "Epoch 12/20, Loss: 0.3673\n",
      "Epoch 13/20, Loss: 0.3673\n",
      "Epoch 14/20, Loss: 0.3638\n",
      "Epoch 15/20, Loss: 0.3609\n",
      "Epoch 16/20, Loss: 0.3610\n",
      "Epoch 17/20, Loss: 0.3618\n",
      "Epoch 18/20, Loss: 0.3604\n",
      "Epoch 19/20, Loss: 0.3629\n",
      "Epoch 20/20, Loss: 0.3592\n",
      "Evaluating MLP with Sigmoid activation function...\n",
      "Accuracy: 0.8146\n",
      "\n",
      "Training MLP with ReLU activation function...\n",
      "Epoch 1/20, Loss: 0.6591\n",
      "Epoch 2/20, Loss: 0.5302\n",
      "Epoch 3/20, Loss: 0.3935\n",
      "Epoch 4/20, Loss: 0.3474\n",
      "Epoch 5/20, Loss: 0.3208\n",
      "Epoch 6/20, Loss: 0.3042\n",
      "Epoch 7/20, Loss: 0.2928\n",
      "Epoch 8/20, Loss: 0.2817\n",
      "Epoch 9/20, Loss: 0.2724\n",
      "Epoch 10/20, Loss: 0.2530\n",
      "Epoch 11/20, Loss: 0.2451\n",
      "Epoch 12/20, Loss: 0.2249\n",
      "Epoch 13/20, Loss: 0.2154\n",
      "Epoch 14/20, Loss: 0.2010\n",
      "Epoch 15/20, Loss: 0.1889\n",
      "Epoch 16/20, Loss: 0.1765\n",
      "Epoch 17/20, Loss: 0.1584\n",
      "Epoch 18/20, Loss: 0.1483\n",
      "Epoch 19/20, Loss: 0.1343\n",
      "Epoch 20/20, Loss: 0.1221\n",
      "Evaluating MLP with ReLU activation function...\n",
      "Accuracy: 0.9463\n",
      "\n",
      "Training MLP with Softmax activation function...\n",
      "Epoch 1/20, Loss: 0.6980\n",
      "Epoch 2/20, Loss: 0.6959\n",
      "Epoch 3/20, Loss: 0.6945\n",
      "Epoch 4/20, Loss: 0.6936\n",
      "Epoch 5/20, Loss: 0.6932\n",
      "Epoch 6/20, Loss: 0.6930\n",
      "Epoch 7/20, Loss: 0.6928\n",
      "Epoch 8/20, Loss: 0.6928\n",
      "Epoch 9/20, Loss: 0.6924\n",
      "Epoch 10/20, Loss: 0.6920\n",
      "Epoch 11/20, Loss: 0.6919\n",
      "Epoch 12/20, Loss: 0.6909\n",
      "Epoch 13/20, Loss: 0.6898\n",
      "Epoch 14/20, Loss: 0.6882\n",
      "Epoch 15/20, Loss: 0.6855\n",
      "Epoch 16/20, Loss: 0.6818\n",
      "Epoch 17/20, Loss: 0.6761\n",
      "Epoch 18/20, Loss: 0.6685\n",
      "Epoch 19/20, Loss: 0.6585\n",
      "Epoch 20/20, Loss: 0.6460\n",
      "Evaluating MLP with Softmax activation function...\n",
      "Accuracy: 0.8244\n",
      "\n",
      "Training MLP with Tanh activation function...\n",
      "Epoch 1/20, Loss: 0.6279\n",
      "Epoch 2/20, Loss: 0.4611\n",
      "Epoch 3/20, Loss: 0.3864\n",
      "Epoch 4/20, Loss: 0.3647\n",
      "Epoch 5/20, Loss: 0.3529\n",
      "Epoch 6/20, Loss: 0.3488\n",
      "Epoch 7/20, Loss: 0.3504\n",
      "Epoch 8/20, Loss: 0.3436\n",
      "Epoch 9/20, Loss: 0.3393\n",
      "Epoch 10/20, Loss: 0.3396\n",
      "Epoch 11/20, Loss: 0.3331\n",
      "Epoch 12/20, Loss: 0.3291\n",
      "Epoch 13/20, Loss: 0.3299\n",
      "Epoch 14/20, Loss: 0.3315\n",
      "Epoch 15/20, Loss: 0.3236\n",
      "Epoch 16/20, Loss: 0.3203\n",
      "Epoch 17/20, Loss: 0.3142\n",
      "Epoch 18/20, Loss: 0.3077\n",
      "Epoch 19/20, Loss: 0.3066\n",
      "Epoch 20/20, Loss: 0.2987\n",
      "Evaluating MLP with Tanh activation function...\n",
      "Accuracy: 0.8780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Linear': 0.8097560975609757,\n",
       " 'Sigmoid': 0.8146341463414634,\n",
       " 'ReLU': 0.9463414634146341,\n",
       " 'Softmax': 0.824390243902439,\n",
       " 'Tanh': 0.8780487804878049}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify the MLPModel class to allow dynamic activation functions\n",
    "class MLPModelWithActivation(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, activation_fn):\n",
    "        super(MLPModelWithActivation, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for neurons in hidden_layers:\n",
    "            layers.append(nn.Linear(current_size, neurons))\n",
    "            layers.append(activation_fn())  # Use the dynamic activation function\n",
    "            current_size = neurons\n",
    "        layers.append(nn.Linear(current_size, 2))  # Output layer (binary classification)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define a function to compare activation functions\n",
    "activation_functions = {\n",
    "    \"Linear\": nn.Identity,  # Linear activation (Identity)\n",
    "    \"Sigmoid\": nn.Sigmoid,\n",
    "    \"ReLU\": nn.ReLU,\n",
    "    \"Softmax\": lambda: nn.Softmax(dim=1),  # Softmax along the output dimension\n",
    "    \"Tanh\": nn.Tanh,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, activation_fn in activation_functions.items():\n",
    "    print(f\"\\nTraining MLP with {name} activation function...\")\n",
    "    model = MLPModelWithActivation(input_size, [16, 32, 64], activation_fn)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_model(model, train_loader, criterion, optimizer, epochs=20)\n",
    "    print(f\"Evaluating MLP with {name} activation function...\")\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Display the results\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIgmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP with Linear activation function...\n",
      "Epoch 1/20, Loss: 0.5676\n",
      "Epoch 2/20, Loss: 0.3865\n",
      "Epoch 3/20, Loss: 0.3676\n",
      "Epoch 4/20, Loss: 0.3612\n",
      "Epoch 5/20, Loss: 0.3595\n",
      "Epoch 6/20, Loss: 0.3570\n",
      "Epoch 7/20, Loss: 0.3662\n",
      "Epoch 8/20, Loss: 0.3584\n",
      "Epoch 9/20, Loss: 0.3547\n",
      "Epoch 10/20, Loss: 0.3564\n",
      "Epoch 11/20, Loss: 0.3543\n",
      "Epoch 12/20, Loss: 0.3593\n",
      "Epoch 13/20, Loss: 0.3593\n",
      "Epoch 14/20, Loss: 0.3583\n",
      "Epoch 15/20, Loss: 0.3617\n",
      "Epoch 16/20, Loss: 0.3558\n",
      "Epoch 17/20, Loss: 0.3557\n",
      "Epoch 18/20, Loss: 0.3567\n",
      "Epoch 19/20, Loss: 0.3546\n",
      "Epoch 20/20, Loss: 0.3579\n",
      "Evaluating MLP with Linear activation function...\n",
      "Accuracy: 0.8244\n",
      "\n",
      "Training MLP with Sigmoid activation function...\n",
      "Epoch 1/20, Loss: 0.7015\n",
      "Epoch 2/20, Loss: 0.6934\n",
      "Epoch 3/20, Loss: 0.6884\n",
      "Epoch 4/20, Loss: 0.6798\n",
      "Epoch 5/20, Loss: 0.6640\n",
      "Epoch 6/20, Loss: 0.6264\n",
      "Epoch 7/20, Loss: 0.5590\n",
      "Epoch 8/20, Loss: 0.4727\n",
      "Epoch 9/20, Loss: 0.4094\n",
      "Epoch 10/20, Loss: 0.3835\n",
      "Epoch 11/20, Loss: 0.3699\n",
      "Epoch 12/20, Loss: 0.3639\n",
      "Epoch 13/20, Loss: 0.3598\n",
      "Epoch 14/20, Loss: 0.3580\n",
      "Epoch 15/20, Loss: 0.3575\n",
      "Epoch 16/20, Loss: 0.3583\n",
      "Epoch 17/20, Loss: 0.3567\n",
      "Epoch 18/20, Loss: 0.3554\n",
      "Epoch 19/20, Loss: 0.3593\n",
      "Epoch 20/20, Loss: 0.3509\n",
      "Evaluating MLP with Sigmoid activation function...\n",
      "Accuracy: 0.8634\n",
      "\n",
      "Training MLP with ReLU activation function...\n",
      "Epoch 1/20, Loss: 0.6727\n",
      "Epoch 2/20, Loss: 0.5745\n",
      "Epoch 3/20, Loss: 0.4167\n",
      "Epoch 4/20, Loss: 0.3652\n",
      "Epoch 5/20, Loss: 0.3450\n",
      "Epoch 6/20, Loss: 0.3205\n",
      "Epoch 7/20, Loss: 0.3061\n",
      "Epoch 8/20, Loss: 0.2900\n",
      "Epoch 9/20, Loss: 0.2767\n",
      "Epoch 10/20, Loss: 0.2625\n",
      "Epoch 11/20, Loss: 0.2513\n",
      "Epoch 12/20, Loss: 0.2386\n",
      "Epoch 13/20, Loss: 0.2230\n",
      "Epoch 14/20, Loss: 0.2089\n",
      "Epoch 15/20, Loss: 0.2015\n",
      "Epoch 16/20, Loss: 0.1850\n",
      "Epoch 17/20, Loss: 0.1725\n",
      "Epoch 18/20, Loss: 0.1580\n",
      "Epoch 19/20, Loss: 0.1493\n",
      "Epoch 20/20, Loss: 0.1410\n",
      "Evaluating MLP with ReLU activation function...\n",
      "Accuracy: 0.9512\n",
      "\n",
      "Training MLP with Softmax activation function...\n",
      "Epoch 1/20, Loss: 0.6938\n",
      "Epoch 2/20, Loss: 0.6936\n",
      "Epoch 3/20, Loss: 0.6932\n",
      "Epoch 4/20, Loss: 0.6932\n",
      "Epoch 5/20, Loss: 0.6930\n",
      "Epoch 6/20, Loss: 0.6928\n",
      "Epoch 7/20, Loss: 0.6928\n",
      "Epoch 8/20, Loss: 0.6924\n",
      "Epoch 9/20, Loss: 0.6925\n",
      "Epoch 10/20, Loss: 0.6922\n",
      "Epoch 11/20, Loss: 0.6914\n",
      "Epoch 12/20, Loss: 0.6908\n",
      "Epoch 13/20, Loss: 0.6894\n",
      "Epoch 14/20, Loss: 0.6874\n",
      "Epoch 15/20, Loss: 0.6841\n",
      "Epoch 16/20, Loss: 0.6791\n",
      "Epoch 17/20, Loss: 0.6724\n",
      "Epoch 18/20, Loss: 0.6630\n",
      "Epoch 19/20, Loss: 0.6512\n",
      "Epoch 20/20, Loss: 0.6368\n",
      "Evaluating MLP with Softmax activation function...\n",
      "Accuracy: 0.8293\n",
      "\n",
      "Training MLP with Tanh activation function...\n",
      "Epoch 1/20, Loss: 0.5831\n",
      "Epoch 2/20, Loss: 0.4100\n",
      "Epoch 3/20, Loss: 0.3727\n",
      "Epoch 4/20, Loss: 0.3619\n",
      "Epoch 5/20, Loss: 0.3595\n",
      "Epoch 6/20, Loss: 0.3530\n",
      "Epoch 7/20, Loss: 0.3517\n",
      "Epoch 8/20, Loss: 0.3498\n",
      "Epoch 9/20, Loss: 0.3466\n",
      "Epoch 10/20, Loss: 0.3430\n",
      "Epoch 11/20, Loss: 0.3439\n",
      "Epoch 12/20, Loss: 0.3413\n",
      "Epoch 13/20, Loss: 0.3376\n",
      "Epoch 14/20, Loss: 0.3332\n",
      "Epoch 15/20, Loss: 0.3331\n",
      "Epoch 16/20, Loss: 0.3311\n",
      "Epoch 17/20, Loss: 0.3317\n",
      "Epoch 18/20, Loss: 0.3246\n",
      "Epoch 19/20, Loss: 0.3222\n",
      "Epoch 20/20, Loss: 0.3147\n",
      "Evaluating MLP with Tanh activation function...\n",
      "Accuracy: 0.8683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Linear': 0.824390243902439,\n",
       " 'Sigmoid': 0.8634146341463415,\n",
       " 'ReLU': 0.9512195121951219,\n",
       " 'Softmax': 0.8292682926829268,\n",
       " 'Tanh': 0.8682926829268293}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine the necessary imports and classes to ensure the environment is ready\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Redefine the class to allow dynamic activation functions\n",
    "class MLPModelWithActivation(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, activation_fn):\n",
    "        super(MLPModelWithActivation, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for neurons in hidden_layers:\n",
    "            layers.append(nn.Linear(current_size, neurons))\n",
    "            layers.append(activation_fn())  # Use the dynamic activation function\n",
    "            current_size = neurons\n",
    "        layers.append(nn.Linear(current_size, 2))  # Output layer (binary classification)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Redefine the training and evaluation functions\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Define activation functions\n",
    "activation_functions = {\n",
    "    \"Linear\": nn.Identity,  # Linear activation (Identity)\n",
    "    \"Sigmoid\": nn.Sigmoid,\n",
    "    \"ReLU\": nn.ReLU,\n",
    "    \"Softmax\": lambda: nn.Softmax(dim=1),  # Softmax along the output dimension\n",
    "    \"Tanh\": nn.Tanh,\n",
    "}\n",
    "\n",
    "# Prepare to store results\n",
    "results = {}\n",
    "\n",
    "# Input size and hidden layers configuration\n",
    "input_size = X_train.shape[1]\n",
    "hidden_layers_config = [16, 32, 64]\n",
    "\n",
    "# Loop through activation functions and train/evaluate models\n",
    "for name, activation_fn in activation_functions.items():\n",
    "    print(f\"\\nTraining MLP with {name} activation function...\")\n",
    "    model = MLPModelWithActivation(input_size, hidden_layers_config, activation_fn)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_model(model, train_loader, nn.CrossEntropyLoss(), optimizer, epochs=20)\n",
    "    print(f\"Evaluating MLP with {name} activation function...\")\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Display the results\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP with ReLU activation for 1 epochs...\n",
      "Epoch 1/1, Loss: 0.6749\n",
      "Evaluating MLP with ReLU activation for 1 epochs...\n",
      "Accuracy: 0.7024\n",
      "\n",
      "Training MLP with ReLU activation for 10 epochs...\n",
      "Epoch 1/10, Loss: 0.6716\n",
      "Epoch 2/10, Loss: 0.5805\n",
      "Epoch 3/10, Loss: 0.4351\n",
      "Epoch 4/10, Loss: 0.3699\n",
      "Epoch 5/10, Loss: 0.3438\n",
      "Epoch 6/10, Loss: 0.3256\n",
      "Epoch 7/10, Loss: 0.3089\n",
      "Epoch 8/10, Loss: 0.2952\n",
      "Epoch 9/10, Loss: 0.2810\n",
      "Epoch 10/10, Loss: 0.2689\n",
      "Evaluating MLP with ReLU activation for 10 epochs...\n",
      "Accuracy: 0.8927\n",
      "\n",
      "Training MLP with ReLU activation for 25 epochs...\n",
      "Epoch 1/25, Loss: 0.6799\n",
      "Epoch 2/25, Loss: 0.6011\n",
      "Epoch 3/25, Loss: 0.4556\n",
      "Epoch 4/25, Loss: 0.3725\n",
      "Epoch 5/25, Loss: 0.3345\n",
      "Epoch 6/25, Loss: 0.3127\n",
      "Epoch 7/25, Loss: 0.2954\n",
      "Epoch 8/25, Loss: 0.2800\n",
      "Epoch 9/25, Loss: 0.2656\n",
      "Epoch 10/25, Loss: 0.2508\n",
      "Epoch 11/25, Loss: 0.2333\n",
      "Epoch 12/25, Loss: 0.2183\n",
      "Epoch 13/25, Loss: 0.2063\n",
      "Epoch 14/25, Loss: 0.1882\n",
      "Epoch 15/25, Loss: 0.1726\n",
      "Epoch 16/25, Loss: 0.1554\n",
      "Epoch 17/25, Loss: 0.1417\n",
      "Epoch 18/25, Loss: 0.1319\n",
      "Epoch 19/25, Loss: 0.1170\n",
      "Epoch 20/25, Loss: 0.1032\n",
      "Epoch 21/25, Loss: 0.0916\n",
      "Epoch 22/25, Loss: 0.0828\n",
      "Epoch 23/25, Loss: 0.0733\n",
      "Epoch 24/25, Loss: 0.0661\n",
      "Epoch 25/25, Loss: 0.0611\n",
      "Evaluating MLP with ReLU activation for 25 epochs...\n",
      "Accuracy: 0.9805\n",
      "\n",
      "Training MLP with ReLU activation for 50 epochs...\n",
      "Epoch 1/50, Loss: 0.6777\n",
      "Epoch 2/50, Loss: 0.5915\n",
      "Epoch 3/50, Loss: 0.4447\n",
      "Epoch 4/50, Loss: 0.3688\n",
      "Epoch 5/50, Loss: 0.3464\n",
      "Epoch 6/50, Loss: 0.3302\n",
      "Epoch 7/50, Loss: 0.3152\n",
      "Epoch 8/50, Loss: 0.2991\n",
      "Epoch 9/50, Loss: 0.2831\n",
      "Epoch 10/50, Loss: 0.2730\n",
      "Epoch 11/50, Loss: 0.2550\n",
      "Epoch 12/50, Loss: 0.2374\n",
      "Epoch 13/50, Loss: 0.2262\n",
      "Epoch 14/50, Loss: 0.2060\n",
      "Epoch 15/50, Loss: 0.1864\n",
      "Epoch 16/50, Loss: 0.1699\n",
      "Epoch 17/50, Loss: 0.1696\n",
      "Epoch 18/50, Loss: 0.1482\n",
      "Epoch 19/50, Loss: 0.1340\n",
      "Epoch 20/50, Loss: 0.1230\n",
      "Epoch 21/50, Loss: 0.1114\n",
      "Epoch 22/50, Loss: 0.1031\n",
      "Epoch 23/50, Loss: 0.0935\n",
      "Epoch 24/50, Loss: 0.0859\n",
      "Epoch 25/50, Loss: 0.0768\n",
      "Epoch 26/50, Loss: 0.0667\n",
      "Epoch 27/50, Loss: 0.0625\n",
      "Epoch 28/50, Loss: 0.0656\n",
      "Epoch 29/50, Loss: 0.0496\n",
      "Epoch 30/50, Loss: 0.0396\n",
      "Epoch 31/50, Loss: 0.0355\n",
      "Epoch 32/50, Loss: 0.0323\n",
      "Epoch 33/50, Loss: 0.0281\n",
      "Epoch 34/50, Loss: 0.0229\n",
      "Epoch 35/50, Loss: 0.0200\n",
      "Epoch 36/50, Loss: 0.0185\n",
      "Epoch 37/50, Loss: 0.0152\n",
      "Epoch 38/50, Loss: 0.0134\n",
      "Epoch 39/50, Loss: 0.0121\n",
      "Epoch 40/50, Loss: 0.0111\n",
      "Epoch 41/50, Loss: 0.0098\n",
      "Epoch 42/50, Loss: 0.0091\n",
      "Epoch 43/50, Loss: 0.0077\n",
      "Epoch 44/50, Loss: 0.0068\n",
      "Epoch 45/50, Loss: 0.0059\n",
      "Epoch 46/50, Loss: 0.0052\n",
      "Epoch 47/50, Loss: 0.0049\n",
      "Epoch 48/50, Loss: 0.0046\n",
      "Epoch 49/50, Loss: 0.0041\n",
      "Epoch 50/50, Loss: 0.0038\n",
      "Evaluating MLP with ReLU activation for 50 epochs...\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Training MLP with ReLU activation for 100 epochs...\n",
      "Epoch 1/100, Loss: 0.6763\n",
      "Epoch 2/100, Loss: 0.5903\n",
      "Epoch 3/100, Loss: 0.4358\n",
      "Epoch 4/100, Loss: 0.3712\n",
      "Epoch 5/100, Loss: 0.3419\n",
      "Epoch 6/100, Loss: 0.3191\n",
      "Epoch 7/100, Loss: 0.3016\n",
      "Epoch 8/100, Loss: 0.2863\n",
      "Epoch 9/100, Loss: 0.2705\n",
      "Epoch 10/100, Loss: 0.2573\n",
      "Epoch 11/100, Loss: 0.2465\n",
      "Epoch 12/100, Loss: 0.2287\n",
      "Epoch 13/100, Loss: 0.2171\n",
      "Epoch 14/100, Loss: 0.2019\n",
      "Epoch 15/100, Loss: 0.1811\n",
      "Epoch 16/100, Loss: 0.1723\n",
      "Epoch 17/100, Loss: 0.1569\n",
      "Epoch 18/100, Loss: 0.1430\n",
      "Epoch 19/100, Loss: 0.1297\n",
      "Epoch 20/100, Loss: 0.1168\n",
      "Epoch 21/100, Loss: 0.1058\n",
      "Epoch 22/100, Loss: 0.0937\n",
      "Epoch 23/100, Loss: 0.0814\n",
      "Epoch 24/100, Loss: 0.0717\n",
      "Epoch 25/100, Loss: 0.0687\n",
      "Epoch 26/100, Loss: 0.0599\n",
      "Epoch 27/100, Loss: 0.0483\n",
      "Epoch 28/100, Loss: 0.0434\n",
      "Epoch 29/100, Loss: 0.0361\n",
      "Epoch 30/100, Loss: 0.0320\n",
      "Epoch 31/100, Loss: 0.0281\n",
      "Epoch 32/100, Loss: 0.0244\n",
      "Epoch 33/100, Loss: 0.0222\n",
      "Epoch 34/100, Loss: 0.0209\n",
      "Epoch 35/100, Loss: 0.0168\n",
      "Epoch 36/100, Loss: 0.0143\n",
      "Epoch 37/100, Loss: 0.0125\n",
      "Epoch 38/100, Loss: 0.0115\n",
      "Epoch 39/100, Loss: 0.0101\n",
      "Epoch 40/100, Loss: 0.0089\n",
      "Epoch 41/100, Loss: 0.0075\n",
      "Epoch 42/100, Loss: 0.0068\n",
      "Epoch 43/100, Loss: 0.0060\n",
      "Epoch 44/100, Loss: 0.0054\n",
      "Epoch 45/100, Loss: 0.0052\n",
      "Epoch 46/100, Loss: 0.0049\n",
      "Epoch 47/100, Loss: 0.0042\n",
      "Epoch 48/100, Loss: 0.0039\n",
      "Epoch 49/100, Loss: 0.0034\n",
      "Epoch 50/100, Loss: 0.0031\n",
      "Epoch 51/100, Loss: 0.0029\n",
      "Epoch 52/100, Loss: 0.0027\n",
      "Epoch 53/100, Loss: 0.0025\n",
      "Epoch 54/100, Loss: 0.0025\n",
      "Epoch 55/100, Loss: 0.0023\n",
      "Epoch 56/100, Loss: 0.0021\n",
      "Epoch 57/100, Loss: 0.0019\n",
      "Epoch 58/100, Loss: 0.0018\n",
      "Epoch 59/100, Loss: 0.0017\n",
      "Epoch 60/100, Loss: 0.0016\n",
      "Epoch 61/100, Loss: 0.0016\n",
      "Epoch 62/100, Loss: 0.0015\n",
      "Epoch 63/100, Loss: 0.0014\n",
      "Epoch 64/100, Loss: 0.0013\n",
      "Epoch 65/100, Loss: 0.0012\n",
      "Epoch 66/100, Loss: 0.0012\n",
      "Epoch 67/100, Loss: 0.0011\n",
      "Epoch 68/100, Loss: 0.0011\n",
      "Epoch 69/100, Loss: 0.0010\n",
      "Epoch 70/100, Loss: 0.0010\n",
      "Epoch 71/100, Loss: 0.0009\n",
      "Epoch 72/100, Loss: 0.0009\n",
      "Epoch 73/100, Loss: 0.0009\n",
      "Epoch 74/100, Loss: 0.0008\n",
      "Epoch 75/100, Loss: 0.0008\n",
      "Epoch 76/100, Loss: 0.0007\n",
      "Epoch 77/100, Loss: 0.0007\n",
      "Epoch 78/100, Loss: 0.0007\n",
      "Epoch 79/100, Loss: 0.0006\n",
      "Epoch 80/100, Loss: 0.0006\n",
      "Epoch 81/100, Loss: 0.0006\n",
      "Epoch 82/100, Loss: 0.0006\n",
      "Epoch 83/100, Loss: 0.0005\n",
      "Epoch 84/100, Loss: 0.0005\n",
      "Epoch 85/100, Loss: 0.0005\n",
      "Epoch 86/100, Loss: 0.0005\n",
      "Epoch 87/100, Loss: 0.0005\n",
      "Epoch 88/100, Loss: 0.0005\n",
      "Epoch 89/100, Loss: 0.0004\n",
      "Epoch 90/100, Loss: 0.0004\n",
      "Epoch 91/100, Loss: 0.0004\n",
      "Epoch 92/100, Loss: 0.0004\n",
      "Epoch 93/100, Loss: 0.0004\n",
      "Epoch 94/100, Loss: 0.0004\n",
      "Epoch 95/100, Loss: 0.0004\n",
      "Epoch 96/100, Loss: 0.0003\n",
      "Epoch 97/100, Loss: 0.0003\n",
      "Epoch 98/100, Loss: 0.0003\n",
      "Epoch 99/100, Loss: 0.0003\n",
      "Epoch 100/100, Loss: 0.0003\n",
      "Evaluating MLP with ReLU activation for 100 epochs...\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Training MLP with ReLU activation for 250 epochs...\n",
      "Epoch 1/250, Loss: 0.6823\n",
      "Epoch 2/250, Loss: 0.6080\n",
      "Epoch 3/250, Loss: 0.4566\n",
      "Epoch 4/250, Loss: 0.3759\n",
      "Epoch 5/250, Loss: 0.3483\n",
      "Epoch 6/250, Loss: 0.3316\n",
      "Epoch 7/250, Loss: 0.3153\n",
      "Epoch 8/250, Loss: 0.2995\n",
      "Epoch 9/250, Loss: 0.2894\n",
      "Epoch 10/250, Loss: 0.2788\n",
      "Epoch 11/250, Loss: 0.2613\n",
      "Epoch 12/250, Loss: 0.2506\n",
      "Epoch 13/250, Loss: 0.2307\n",
      "Epoch 14/250, Loss: 0.2229\n",
      "Epoch 15/250, Loss: 0.2092\n",
      "Epoch 16/250, Loss: 0.1898\n",
      "Epoch 17/250, Loss: 0.1752\n",
      "Epoch 18/250, Loss: 0.1716\n",
      "Epoch 19/250, Loss: 0.1551\n",
      "Epoch 20/250, Loss: 0.1369\n",
      "Epoch 21/250, Loss: 0.1211\n",
      "Epoch 22/250, Loss: 0.1123\n",
      "Epoch 23/250, Loss: 0.1019\n",
      "Epoch 24/250, Loss: 0.0955\n",
      "Epoch 25/250, Loss: 0.0824\n",
      "Epoch 26/250, Loss: 0.0735\n",
      "Epoch 27/250, Loss: 0.0673\n",
      "Epoch 28/250, Loss: 0.0576\n",
      "Epoch 29/250, Loss: 0.0504\n",
      "Epoch 30/250, Loss: 0.0467\n",
      "Epoch 31/250, Loss: 0.0430\n",
      "Epoch 32/250, Loss: 0.0374\n",
      "Epoch 33/250, Loss: 0.0329\n",
      "Epoch 34/250, Loss: 0.0300\n",
      "Epoch 35/250, Loss: 0.0271\n",
      "Epoch 36/250, Loss: 0.0251\n",
      "Epoch 37/250, Loss: 0.0236\n",
      "Epoch 38/250, Loss: 0.0196\n",
      "Epoch 39/250, Loss: 0.0192\n",
      "Epoch 40/250, Loss: 0.0168\n",
      "Epoch 41/250, Loss: 0.0147\n",
      "Epoch 42/250, Loss: 0.0136\n",
      "Epoch 43/250, Loss: 0.0124\n",
      "Epoch 44/250, Loss: 0.0111\n",
      "Epoch 45/250, Loss: 0.0113\n",
      "Epoch 46/250, Loss: 0.0115\n",
      "Epoch 47/250, Loss: 0.0091\n",
      "Epoch 48/250, Loss: 0.0080\n",
      "Epoch 49/250, Loss: 0.0075\n",
      "Epoch 50/250, Loss: 0.0069\n",
      "Epoch 51/250, Loss: 0.0061\n",
      "Epoch 52/250, Loss: 0.0056\n",
      "Epoch 53/250, Loss: 0.0051\n",
      "Epoch 54/250, Loss: 0.0050\n",
      "Epoch 55/250, Loss: 0.0046\n",
      "Epoch 56/250, Loss: 0.0040\n",
      "Epoch 57/250, Loss: 0.0038\n",
      "Epoch 58/250, Loss: 0.0036\n",
      "Epoch 59/250, Loss: 0.0032\n",
      "Epoch 60/250, Loss: 0.0030\n",
      "Epoch 61/250, Loss: 0.0029\n",
      "Epoch 62/250, Loss: 0.0028\n",
      "Epoch 63/250, Loss: 0.0026\n",
      "Epoch 64/250, Loss: 0.0022\n",
      "Epoch 65/250, Loss: 0.0021\n",
      "Epoch 66/250, Loss: 0.0019\n",
      "Epoch 67/250, Loss: 0.0018\n",
      "Epoch 68/250, Loss: 0.0017\n",
      "Epoch 69/250, Loss: 0.0017\n",
      "Epoch 70/250, Loss: 0.0016\n",
      "Epoch 71/250, Loss: 0.0014\n",
      "Epoch 72/250, Loss: 0.0014\n",
      "Epoch 73/250, Loss: 0.0013\n",
      "Epoch 74/250, Loss: 0.0012\n",
      "Epoch 75/250, Loss: 0.0012\n",
      "Epoch 76/250, Loss: 0.0011\n",
      "Epoch 77/250, Loss: 0.0011\n",
      "Epoch 78/250, Loss: 0.0010\n",
      "Epoch 79/250, Loss: 0.0009\n",
      "Epoch 80/250, Loss: 0.0009\n",
      "Epoch 81/250, Loss: 0.0009\n",
      "Epoch 82/250, Loss: 0.0008\n",
      "Epoch 83/250, Loss: 0.0008\n",
      "Epoch 84/250, Loss: 0.0008\n",
      "Epoch 85/250, Loss: 0.0007\n",
      "Epoch 86/250, Loss: 0.0007\n",
      "Epoch 87/250, Loss: 0.0007\n",
      "Epoch 88/250, Loss: 0.0006\n",
      "Epoch 89/250, Loss: 0.0006\n",
      "Epoch 90/250, Loss: 0.0006\n",
      "Epoch 91/250, Loss: 0.0006\n",
      "Epoch 92/250, Loss: 0.0006\n",
      "Epoch 93/250, Loss: 0.0005\n",
      "Epoch 94/250, Loss: 0.0005\n",
      "Epoch 95/250, Loss: 0.0005\n",
      "Epoch 96/250, Loss: 0.0005\n",
      "Epoch 97/250, Loss: 0.0004\n",
      "Epoch 98/250, Loss: 0.0004\n",
      "Epoch 99/250, Loss: 0.0004\n",
      "Epoch 100/250, Loss: 0.0004\n",
      "Epoch 101/250, Loss: 0.0004\n",
      "Epoch 102/250, Loss: 0.0004\n",
      "Epoch 103/250, Loss: 0.0004\n",
      "Epoch 104/250, Loss: 0.0003\n",
      "Epoch 105/250, Loss: 0.0003\n",
      "Epoch 106/250, Loss: 0.0003\n",
      "Epoch 107/250, Loss: 0.0003\n",
      "Epoch 108/250, Loss: 0.0003\n",
      "Epoch 109/250, Loss: 0.0003\n",
      "Epoch 110/250, Loss: 0.0003\n",
      "Epoch 111/250, Loss: 0.0003\n",
      "Epoch 112/250, Loss: 0.0003\n",
      "Epoch 113/250, Loss: 0.0003\n",
      "Epoch 114/250, Loss: 0.0002\n",
      "Epoch 115/250, Loss: 0.0002\n",
      "Epoch 116/250, Loss: 0.0002\n",
      "Epoch 117/250, Loss: 0.0002\n",
      "Epoch 118/250, Loss: 0.0002\n",
      "Epoch 119/250, Loss: 0.0002\n",
      "Epoch 120/250, Loss: 0.0002\n",
      "Epoch 121/250, Loss: 0.0002\n",
      "Epoch 122/250, Loss: 0.0002\n",
      "Epoch 123/250, Loss: 0.0002\n",
      "Epoch 124/250, Loss: 0.0002\n",
      "Epoch 125/250, Loss: 0.0002\n",
      "Epoch 126/250, Loss: 0.0002\n",
      "Epoch 127/250, Loss: 0.0002\n",
      "Epoch 128/250, Loss: 0.0002\n",
      "Epoch 129/250, Loss: 0.0002\n",
      "Epoch 130/250, Loss: 0.0002\n",
      "Epoch 131/250, Loss: 0.0002\n",
      "Epoch 132/250, Loss: 0.0001\n",
      "Epoch 133/250, Loss: 0.0001\n",
      "Epoch 134/250, Loss: 0.0001\n",
      "Epoch 135/250, Loss: 0.0001\n",
      "Epoch 136/250, Loss: 0.0001\n",
      "Epoch 137/250, Loss: 0.0001\n",
      "Epoch 138/250, Loss: 0.0001\n",
      "Epoch 139/250, Loss: 0.0001\n",
      "Epoch 140/250, Loss: 0.0001\n",
      "Epoch 141/250, Loss: 0.0001\n",
      "Epoch 142/250, Loss: 0.0001\n",
      "Epoch 143/250, Loss: 0.0001\n",
      "Epoch 144/250, Loss: 0.0001\n",
      "Epoch 145/250, Loss: 0.0001\n",
      "Epoch 146/250, Loss: 0.0001\n",
      "Epoch 147/250, Loss: 0.0001\n",
      "Epoch 148/250, Loss: 0.0001\n",
      "Epoch 149/250, Loss: 0.0001\n",
      "Epoch 150/250, Loss: 0.0001\n",
      "Epoch 151/250, Loss: 0.0001\n",
      "Epoch 152/250, Loss: 0.0001\n",
      "Epoch 153/250, Loss: 0.0001\n",
      "Epoch 154/250, Loss: 0.0001\n",
      "Epoch 155/250, Loss: 0.0001\n",
      "Epoch 156/250, Loss: 0.0001\n",
      "Epoch 157/250, Loss: 0.0001\n",
      "Epoch 158/250, Loss: 0.0001\n",
      "Epoch 159/250, Loss: 0.0001\n",
      "Epoch 160/250, Loss: 0.0001\n",
      "Epoch 161/250, Loss: 0.0001\n",
      "Epoch 162/250, Loss: 0.0001\n",
      "Epoch 163/250, Loss: 0.0001\n",
      "Epoch 164/250, Loss: 0.0001\n",
      "Epoch 165/250, Loss: 0.0001\n",
      "Epoch 166/250, Loss: 0.0001\n",
      "Epoch 167/250, Loss: 0.0001\n",
      "Epoch 168/250, Loss: 0.0001\n",
      "Epoch 169/250, Loss: 0.0001\n",
      "Epoch 170/250, Loss: 0.0001\n",
      "Epoch 171/250, Loss: 0.0001\n",
      "Epoch 172/250, Loss: 0.0001\n",
      "Epoch 173/250, Loss: 0.0001\n",
      "Epoch 174/250, Loss: 0.0001\n",
      "Epoch 175/250, Loss: 0.0001\n",
      "Epoch 176/250, Loss: 0.0001\n",
      "Epoch 177/250, Loss: 0.0001\n",
      "Epoch 178/250, Loss: 0.0001\n",
      "Epoch 179/250, Loss: 0.0001\n",
      "Epoch 180/250, Loss: 0.0000\n",
      "Epoch 181/250, Loss: 0.0000\n",
      "Epoch 182/250, Loss: 0.0000\n",
      "Epoch 183/250, Loss: 0.0000\n",
      "Epoch 184/250, Loss: 0.0000\n",
      "Epoch 185/250, Loss: 0.0000\n",
      "Epoch 186/250, Loss: 0.0000\n",
      "Epoch 187/250, Loss: 0.0000\n",
      "Epoch 188/250, Loss: 0.0000\n",
      "Epoch 189/250, Loss: 0.0000\n",
      "Epoch 190/250, Loss: 0.0000\n",
      "Epoch 191/250, Loss: 0.0000\n",
      "Epoch 192/250, Loss: 0.0000\n",
      "Epoch 193/250, Loss: 0.0000\n",
      "Epoch 194/250, Loss: 0.0000\n",
      "Epoch 195/250, Loss: 0.0000\n",
      "Epoch 196/250, Loss: 0.0000\n",
      "Epoch 197/250, Loss: 0.0000\n",
      "Epoch 198/250, Loss: 0.0000\n",
      "Epoch 199/250, Loss: 0.0000\n",
      "Epoch 200/250, Loss: 0.0000\n",
      "Epoch 201/250, Loss: 0.0000\n",
      "Epoch 202/250, Loss: 0.0000\n",
      "Epoch 203/250, Loss: 0.0000\n",
      "Epoch 204/250, Loss: 0.0000\n",
      "Epoch 205/250, Loss: 0.0000\n",
      "Epoch 206/250, Loss: 0.0000\n",
      "Epoch 207/250, Loss: 0.0000\n",
      "Epoch 208/250, Loss: 0.0000\n",
      "Epoch 209/250, Loss: 0.0000\n",
      "Epoch 210/250, Loss: 0.0000\n",
      "Epoch 211/250, Loss: 0.0000\n",
      "Epoch 212/250, Loss: 0.0000\n",
      "Epoch 213/250, Loss: 0.0000\n",
      "Epoch 214/250, Loss: 0.0000\n",
      "Epoch 215/250, Loss: 0.0000\n",
      "Epoch 216/250, Loss: 0.0000\n",
      "Epoch 217/250, Loss: 0.0000\n",
      "Epoch 218/250, Loss: 0.0000\n",
      "Epoch 219/250, Loss: 0.0000\n",
      "Epoch 220/250, Loss: 0.0000\n",
      "Epoch 221/250, Loss: 0.0000\n",
      "Epoch 222/250, Loss: 0.0000\n",
      "Epoch 223/250, Loss: 0.0000\n",
      "Epoch 224/250, Loss: 0.0000\n",
      "Epoch 225/250, Loss: 0.0000\n",
      "Epoch 226/250, Loss: 0.0000\n",
      "Epoch 227/250, Loss: 0.0000\n",
      "Epoch 228/250, Loss: 0.0000\n",
      "Epoch 229/250, Loss: 0.0000\n",
      "Epoch 230/250, Loss: 0.0000\n",
      "Epoch 231/250, Loss: 0.0000\n",
      "Epoch 232/250, Loss: 0.0000\n",
      "Epoch 233/250, Loss: 0.0000\n",
      "Epoch 234/250, Loss: 0.0000\n",
      "Epoch 235/250, Loss: 0.0000\n",
      "Epoch 236/250, Loss: 0.0000\n",
      "Epoch 237/250, Loss: 0.0000\n",
      "Epoch 238/250, Loss: 0.0000\n",
      "Epoch 239/250, Loss: 0.0000\n",
      "Epoch 240/250, Loss: 0.0000\n",
      "Epoch 241/250, Loss: 0.0000\n",
      "Epoch 242/250, Loss: 0.0000\n",
      "Epoch 243/250, Loss: 0.0000\n",
      "Epoch 244/250, Loss: 0.0000\n",
      "Epoch 245/250, Loss: 0.0000\n",
      "Epoch 246/250, Loss: 0.0000\n",
      "Epoch 247/250, Loss: 0.0000\n",
      "Epoch 248/250, Loss: 0.0000\n",
      "Epoch 249/250, Loss: 0.0000\n",
      "Epoch 250/250, Loss: 0.0000\n",
      "Evaluating MLP with ReLU activation for 250 epochs...\n",
      "Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.7024390243902439,\n",
       " 10: 0.8926829268292683,\n",
       " 25: 0.9804878048780488,\n",
       " 50: 1.0,\n",
       " 100: 1.0,\n",
       " 250: 1.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the epochs to test\n",
    "epoch_values = [1, 10, 25, 50, 100, 250]\n",
    "epoch_results = {}\n",
    "\n",
    "# Use the same model configuration with ReLU activation\n",
    "for epochs in epoch_values:\n",
    "    print(f\"\\nTraining MLP with ReLU activation for {epochs} epochs...\")\n",
    "    model = MLPModelWithActivation(input_size, [16, 32, 64], nn.ReLU)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, nn.CrossEntropyLoss(), optimizer, epochs=epochs)\n",
    "    print(f\"Evaluating MLP with ReLU activation for {epochs} epochs...\")\n",
    "    \n",
    "    # Evaluate and store the accuracy\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    epoch_results[epochs] = accuracy\n",
    "\n",
    "# Display the results\n",
    "epoch_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP with ReLU activation for 1 epochs...\n",
      "Epoch 1/1, Loss: 0.6656\n",
      "Evaluating MLP with ReLU activation for 1 epochs...\n",
      "Accuracy: 0.7902\n",
      "\n",
      "Training MLP with ReLU activation for 10 epochs...\n",
      "Epoch 1/10, Loss: 0.6712\n",
      "Epoch 2/10, Loss: 0.5532\n",
      "Epoch 3/10, Loss: 0.3847\n",
      "Epoch 4/10, Loss: 0.3421\n",
      "Epoch 5/10, Loss: 0.3221\n",
      "Epoch 6/10, Loss: 0.2999\n",
      "Epoch 7/10, Loss: 0.2900\n",
      "Epoch 8/10, Loss: 0.2783\n",
      "Epoch 9/10, Loss: 0.2636\n",
      "Epoch 10/10, Loss: 0.2496\n",
      "Evaluating MLP with ReLU activation for 10 epochs...\n",
      "Accuracy: 0.8683\n",
      "\n",
      "Training MLP with ReLU activation for 25 epochs...\n",
      "Epoch 1/25, Loss: 0.6742\n",
      "Epoch 2/25, Loss: 0.5469\n",
      "Epoch 3/25, Loss: 0.4020\n",
      "Epoch 4/25, Loss: 0.3656\n",
      "Epoch 5/25, Loss: 0.3418\n",
      "Epoch 6/25, Loss: 0.3273\n",
      "Epoch 7/25, Loss: 0.3183\n",
      "Epoch 8/25, Loss: 0.3088\n",
      "Epoch 9/25, Loss: 0.2966\n",
      "Epoch 10/25, Loss: 0.2854\n",
      "Epoch 11/25, Loss: 0.2761\n",
      "Epoch 12/25, Loss: 0.2659\n",
      "Epoch 13/25, Loss: 0.2566\n",
      "Epoch 14/25, Loss: 0.2416\n",
      "Epoch 15/25, Loss: 0.2266\n",
      "Epoch 16/25, Loss: 0.2154\n",
      "Epoch 17/25, Loss: 0.2121\n",
      "Epoch 18/25, Loss: 0.1962\n",
      "Epoch 19/25, Loss: 0.1828\n",
      "Epoch 20/25, Loss: 0.1690\n",
      "Epoch 21/25, Loss: 0.1572\n",
      "Epoch 22/25, Loss: 0.1453\n",
      "Epoch 23/25, Loss: 0.1321\n",
      "Epoch 24/25, Loss: 0.1181\n",
      "Epoch 25/25, Loss: 0.1032\n",
      "Evaluating MLP with ReLU activation for 25 epochs...\n",
      "Accuracy: 0.9512\n",
      "\n",
      "Training MLP with ReLU activation for 50 epochs...\n",
      "Epoch 1/50, Loss: 0.6539\n",
      "Epoch 2/50, Loss: 0.5292\n",
      "Epoch 3/50, Loss: 0.4152\n",
      "Epoch 4/50, Loss: 0.3613\n",
      "Epoch 5/50, Loss: 0.3381\n",
      "Epoch 6/50, Loss: 0.3160\n",
      "Epoch 7/50, Loss: 0.3048\n",
      "Epoch 8/50, Loss: 0.2940\n",
      "Epoch 9/50, Loss: 0.2843\n",
      "Epoch 10/50, Loss: 0.2714\n",
      "Epoch 11/50, Loss: 0.2680\n",
      "Epoch 12/50, Loss: 0.2552\n",
      "Epoch 13/50, Loss: 0.2435\n",
      "Epoch 14/50, Loss: 0.2334\n",
      "Epoch 15/50, Loss: 0.2259\n",
      "Epoch 16/50, Loss: 0.2170\n",
      "Epoch 17/50, Loss: 0.2062\n",
      "Epoch 18/50, Loss: 0.1939\n",
      "Epoch 19/50, Loss: 0.1809\n",
      "Epoch 20/50, Loss: 0.1702\n",
      "Epoch 21/50, Loss: 0.1620\n",
      "Epoch 22/50, Loss: 0.1530\n",
      "Epoch 23/50, Loss: 0.1461\n",
      "Epoch 24/50, Loss: 0.1368\n",
      "Epoch 25/50, Loss: 0.1217\n",
      "Epoch 26/50, Loss: 0.1138\n",
      "Epoch 27/50, Loss: 0.1007\n",
      "Epoch 28/50, Loss: 0.0959\n",
      "Epoch 29/50, Loss: 0.0850\n",
      "Epoch 30/50, Loss: 0.0772\n",
      "Epoch 31/50, Loss: 0.0727\n",
      "Epoch 32/50, Loss: 0.0634\n",
      "Epoch 33/50, Loss: 0.0603\n",
      "Epoch 34/50, Loss: 0.0517\n",
      "Epoch 35/50, Loss: 0.0485\n",
      "Epoch 36/50, Loss: 0.0417\n",
      "Epoch 37/50, Loss: 0.0413\n",
      "Epoch 38/50, Loss: 0.0349\n",
      "Epoch 39/50, Loss: 0.0290\n",
      "Epoch 40/50, Loss: 0.0269\n",
      "Epoch 41/50, Loss: 0.0232\n",
      "Epoch 42/50, Loss: 0.0230\n",
      "Epoch 43/50, Loss: 0.0186\n",
      "Epoch 44/50, Loss: 0.0179\n",
      "Epoch 45/50, Loss: 0.0151\n",
      "Epoch 46/50, Loss: 0.0138\n",
      "Epoch 47/50, Loss: 0.0121\n",
      "Epoch 48/50, Loss: 0.0109\n",
      "Epoch 49/50, Loss: 0.0098\n",
      "Epoch 50/50, Loss: 0.0100\n",
      "Evaluating MLP with ReLU activation for 50 epochs...\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Training MLP with ReLU activation for 100 epochs...\n",
      "Epoch 1/100, Loss: 0.6591\n",
      "Epoch 2/100, Loss: 0.5396\n",
      "Epoch 3/100, Loss: 0.3905\n",
      "Epoch 4/100, Loss: 0.3374\n",
      "Epoch 5/100, Loss: 0.3181\n",
      "Epoch 6/100, Loss: 0.3067\n",
      "Epoch 7/100, Loss: 0.2920\n",
      "Epoch 8/100, Loss: 0.2776\n",
      "Epoch 9/100, Loss: 0.2708\n",
      "Epoch 10/100, Loss: 0.2602\n",
      "Epoch 11/100, Loss: 0.2466\n",
      "Epoch 12/100, Loss: 0.2299\n",
      "Epoch 13/100, Loss: 0.2272\n",
      "Epoch 14/100, Loss: 0.2166\n",
      "Epoch 15/100, Loss: 0.1926\n",
      "Epoch 16/100, Loss: 0.1842\n",
      "Epoch 17/100, Loss: 0.1723\n",
      "Epoch 18/100, Loss: 0.1620\n",
      "Epoch 19/100, Loss: 0.1455\n",
      "Epoch 20/100, Loss: 0.1372\n",
      "Epoch 21/100, Loss: 0.1269\n",
      "Epoch 22/100, Loss: 0.1148\n",
      "Epoch 23/100, Loss: 0.1050\n",
      "Epoch 24/100, Loss: 0.0971\n",
      "Epoch 25/100, Loss: 0.0897\n",
      "Epoch 26/100, Loss: 0.0869\n",
      "Epoch 27/100, Loss: 0.0761\n",
      "Epoch 28/100, Loss: 0.0687\n",
      "Epoch 29/100, Loss: 0.0621\n",
      "Epoch 30/100, Loss: 0.0584\n",
      "Epoch 31/100, Loss: 0.0564\n",
      "Epoch 32/100, Loss: 0.0487\n",
      "Epoch 33/100, Loss: 0.0459\n",
      "Epoch 34/100, Loss: 0.0423\n",
      "Epoch 35/100, Loss: 0.0389\n",
      "Epoch 36/100, Loss: 0.0368\n",
      "Epoch 37/100, Loss: 0.0344\n",
      "Epoch 38/100, Loss: 0.0307\n",
      "Epoch 39/100, Loss: 0.0279\n",
      "Epoch 40/100, Loss: 0.0288\n",
      "Epoch 41/100, Loss: 0.0247\n",
      "Epoch 42/100, Loss: 0.0224\n",
      "Epoch 43/100, Loss: 0.0208\n",
      "Epoch 44/100, Loss: 0.0198\n",
      "Epoch 45/100, Loss: 0.0184\n",
      "Epoch 46/100, Loss: 0.0170\n",
      "Epoch 47/100, Loss: 0.0155\n",
      "Epoch 48/100, Loss: 0.0154\n",
      "Epoch 49/100, Loss: 0.0139\n",
      "Epoch 50/100, Loss: 0.0128\n",
      "Epoch 51/100, Loss: 0.0117\n",
      "Epoch 52/100, Loss: 0.0114\n",
      "Epoch 53/100, Loss: 0.0109\n",
      "Epoch 54/100, Loss: 0.0098\n",
      "Epoch 55/100, Loss: 0.0093\n",
      "Epoch 56/100, Loss: 0.0093\n",
      "Epoch 57/100, Loss: 0.0081\n",
      "Epoch 58/100, Loss: 0.0083\n",
      "Epoch 59/100, Loss: 0.0071\n",
      "Epoch 60/100, Loss: 0.0062\n",
      "Epoch 61/100, Loss: 0.0059\n",
      "Epoch 62/100, Loss: 0.0069\n",
      "Epoch 63/100, Loss: 0.0066\n",
      "Epoch 64/100, Loss: 0.0048\n",
      "Epoch 65/100, Loss: 0.0043\n",
      "Epoch 66/100, Loss: 0.0039\n",
      "Epoch 67/100, Loss: 0.0036\n",
      "Epoch 68/100, Loss: 0.0033\n",
      "Epoch 69/100, Loss: 0.0031\n",
      "Epoch 70/100, Loss: 0.0028\n",
      "Epoch 71/100, Loss: 0.0025\n",
      "Epoch 72/100, Loss: 0.0024\n",
      "Epoch 73/100, Loss: 0.0022\n",
      "Epoch 74/100, Loss: 0.0020\n",
      "Epoch 75/100, Loss: 0.0019\n",
      "Epoch 76/100, Loss: 0.0017\n",
      "Epoch 77/100, Loss: 0.0018\n",
      "Epoch 78/100, Loss: 0.0016\n",
      "Epoch 79/100, Loss: 0.0015\n",
      "Epoch 80/100, Loss: 0.0014\n",
      "Epoch 81/100, Loss: 0.0013\n",
      "Epoch 82/100, Loss: 0.0013\n",
      "Epoch 83/100, Loss: 0.0012\n",
      "Epoch 84/100, Loss: 0.0011\n",
      "Epoch 85/100, Loss: 0.0011\n",
      "Epoch 86/100, Loss: 0.0010\n",
      "Epoch 87/100, Loss: 0.0010\n",
      "Epoch 88/100, Loss: 0.0009\n",
      "Epoch 89/100, Loss: 0.0009\n",
      "Epoch 90/100, Loss: 0.0008\n",
      "Epoch 91/100, Loss: 0.0008\n",
      "Epoch 92/100, Loss: 0.0008\n",
      "Epoch 93/100, Loss: 0.0007\n",
      "Epoch 94/100, Loss: 0.0007\n",
      "Epoch 95/100, Loss: 0.0007\n",
      "Epoch 96/100, Loss: 0.0007\n",
      "Epoch 97/100, Loss: 0.0006\n",
      "Epoch 98/100, Loss: 0.0006\n",
      "Epoch 99/100, Loss: 0.0006\n",
      "Epoch 100/100, Loss: 0.0006\n",
      "Evaluating MLP with ReLU activation for 100 epochs...\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Training MLP with ReLU activation for 250 epochs...\n",
      "Epoch 1/250, Loss: 0.6384\n",
      "Epoch 2/250, Loss: 0.4911\n",
      "Epoch 3/250, Loss: 0.3693\n",
      "Epoch 4/250, Loss: 0.3383\n",
      "Epoch 5/250, Loss: 0.3155\n",
      "Epoch 6/250, Loss: 0.3019\n",
      "Epoch 7/250, Loss: 0.2928\n",
      "Epoch 8/250, Loss: 0.2797\n",
      "Epoch 9/250, Loss: 0.2689\n",
      "Epoch 10/250, Loss: 0.2527\n",
      "Epoch 11/250, Loss: 0.2356\n",
      "Epoch 12/250, Loss: 0.2194\n",
      "Epoch 13/250, Loss: 0.2028\n",
      "Epoch 14/250, Loss: 0.1914\n",
      "Epoch 15/250, Loss: 0.1667\n",
      "Epoch 16/250, Loss: 0.1537\n",
      "Epoch 17/250, Loss: 0.1374\n",
      "Epoch 18/250, Loss: 0.1216\n",
      "Epoch 19/250, Loss: 0.1088\n",
      "Epoch 20/250, Loss: 0.0945\n",
      "Epoch 21/250, Loss: 0.0884\n",
      "Epoch 22/250, Loss: 0.0746\n",
      "Epoch 23/250, Loss: 0.0637\n",
      "Epoch 24/250, Loss: 0.0556\n",
      "Epoch 25/250, Loss: 0.0476\n",
      "Epoch 26/250, Loss: 0.0409\n",
      "Epoch 27/250, Loss: 0.0351\n",
      "Epoch 28/250, Loss: 0.0298\n",
      "Epoch 29/250, Loss: 0.0257\n",
      "Epoch 30/250, Loss: 0.0247\n",
      "Epoch 31/250, Loss: 0.0200\n",
      "Epoch 32/250, Loss: 0.0182\n",
      "Epoch 33/250, Loss: 0.0146\n",
      "Epoch 34/250, Loss: 0.0137\n",
      "Epoch 35/250, Loss: 0.0114\n",
      "Epoch 36/250, Loss: 0.0104\n",
      "Epoch 37/250, Loss: 0.0103\n",
      "Epoch 38/250, Loss: 0.0087\n",
      "Epoch 39/250, Loss: 0.0083\n",
      "Epoch 40/250, Loss: 0.0072\n",
      "Epoch 41/250, Loss: 0.0065\n",
      "Epoch 42/250, Loss: 0.0060\n",
      "Epoch 43/250, Loss: 0.0060\n",
      "Epoch 44/250, Loss: 0.0051\n",
      "Epoch 45/250, Loss: 0.0050\n",
      "Epoch 46/250, Loss: 0.0052\n",
      "Epoch 47/250, Loss: 0.0041\n",
      "Epoch 48/250, Loss: 0.0040\n",
      "Epoch 49/250, Loss: 0.0038\n",
      "Epoch 50/250, Loss: 0.0042\n",
      "Epoch 51/250, Loss: 0.0033\n",
      "Epoch 52/250, Loss: 0.0028\n",
      "Epoch 53/250, Loss: 0.0027\n",
      "Epoch 54/250, Loss: 0.0025\n",
      "Epoch 55/250, Loss: 0.0024\n",
      "Epoch 56/250, Loss: 0.0022\n",
      "Epoch 57/250, Loss: 0.0021\n",
      "Epoch 58/250, Loss: 0.0020\n",
      "Epoch 59/250, Loss: 0.0020\n",
      "Epoch 60/250, Loss: 0.0019\n",
      "Epoch 61/250, Loss: 0.0017\n",
      "Epoch 62/250, Loss: 0.0016\n",
      "Epoch 63/250, Loss: 0.0015\n",
      "Epoch 64/250, Loss: 0.0015\n",
      "Epoch 65/250, Loss: 0.0014\n",
      "Epoch 66/250, Loss: 0.0013\n",
      "Epoch 67/250, Loss: 0.0012\n",
      "Epoch 68/250, Loss: 0.0013\n",
      "Epoch 69/250, Loss: 0.0012\n",
      "Epoch 70/250, Loss: 0.0011\n",
      "Epoch 71/250, Loss: 0.0010\n",
      "Epoch 72/250, Loss: 0.0010\n",
      "Epoch 73/250, Loss: 0.0009\n",
      "Epoch 74/250, Loss: 0.0009\n",
      "Epoch 75/250, Loss: 0.0009\n",
      "Epoch 76/250, Loss: 0.0008\n",
      "Epoch 77/250, Loss: 0.0009\n",
      "Epoch 78/250, Loss: 0.0007\n",
      "Epoch 79/250, Loss: 0.0007\n",
      "Epoch 80/250, Loss: 0.0007\n",
      "Epoch 81/250, Loss: 0.0007\n",
      "Epoch 82/250, Loss: 0.0006\n",
      "Epoch 83/250, Loss: 0.0006\n",
      "Epoch 84/250, Loss: 0.0006\n",
      "Epoch 85/250, Loss: 0.0005\n",
      "Epoch 86/250, Loss: 0.0005\n",
      "Epoch 87/250, Loss: 0.0005\n",
      "Epoch 88/250, Loss: 0.0005\n",
      "Epoch 89/250, Loss: 0.0005\n",
      "Epoch 90/250, Loss: 0.0004\n",
      "Epoch 91/250, Loss: 0.0004\n",
      "Epoch 92/250, Loss: 0.0004\n",
      "Epoch 93/250, Loss: 0.0004\n",
      "Epoch 94/250, Loss: 0.0003\n",
      "Epoch 95/250, Loss: 0.0003\n",
      "Epoch 96/250, Loss: 0.0003\n",
      "Epoch 97/250, Loss: 0.0003\n",
      "Epoch 98/250, Loss: 0.0003\n",
      "Epoch 99/250, Loss: 0.0003\n",
      "Epoch 100/250, Loss: 0.0003\n",
      "Epoch 101/250, Loss: 0.0002\n",
      "Epoch 102/250, Loss: 0.0002\n",
      "Epoch 103/250, Loss: 0.0002\n",
      "Epoch 104/250, Loss: 0.0002\n",
      "Epoch 105/250, Loss: 0.0002\n",
      "Epoch 106/250, Loss: 0.0002\n",
      "Epoch 107/250, Loss: 0.0002\n",
      "Epoch 108/250, Loss: 0.0002\n",
      "Epoch 109/250, Loss: 0.0001\n",
      "Epoch 110/250, Loss: 0.0001\n",
      "Epoch 111/250, Loss: 0.0001\n",
      "Epoch 112/250, Loss: 0.0001\n",
      "Epoch 113/250, Loss: 0.0001\n",
      "Epoch 114/250, Loss: 0.0001\n",
      "Epoch 115/250, Loss: 0.0001\n",
      "Epoch 116/250, Loss: 0.0001\n",
      "Epoch 117/250, Loss: 0.0001\n",
      "Epoch 118/250, Loss: 0.0001\n",
      "Epoch 119/250, Loss: 0.0001\n",
      "Epoch 120/250, Loss: 0.0001\n",
      "Epoch 121/250, Loss: 0.0001\n",
      "Epoch 122/250, Loss: 0.0001\n",
      "Epoch 123/250, Loss: 0.0001\n",
      "Epoch 124/250, Loss: 0.0001\n",
      "Epoch 125/250, Loss: 0.0001\n",
      "Epoch 126/250, Loss: 0.0001\n",
      "Epoch 127/250, Loss: 0.0001\n",
      "Epoch 128/250, Loss: 0.0001\n",
      "Epoch 129/250, Loss: 0.0001\n",
      "Epoch 130/250, Loss: 0.0001\n",
      "Epoch 131/250, Loss: 0.0001\n",
      "Epoch 132/250, Loss: 0.0001\n",
      "Epoch 133/250, Loss: 0.0000\n",
      "Epoch 134/250, Loss: 0.0000\n",
      "Epoch 135/250, Loss: 0.0000\n",
      "Epoch 136/250, Loss: 0.0000\n",
      "Epoch 137/250, Loss: 0.0000\n",
      "Epoch 138/250, Loss: 0.0000\n",
      "Epoch 139/250, Loss: 0.0000\n",
      "Epoch 140/250, Loss: 0.0000\n",
      "Epoch 141/250, Loss: 0.0000\n",
      "Epoch 142/250, Loss: 0.0000\n",
      "Epoch 143/250, Loss: 0.0000\n",
      "Epoch 144/250, Loss: 0.0000\n",
      "Epoch 145/250, Loss: 0.0000\n",
      "Epoch 146/250, Loss: 0.0000\n",
      "Epoch 147/250, Loss: 0.0000\n",
      "Epoch 148/250, Loss: 0.0000\n",
      "Epoch 149/250, Loss: 0.0000\n",
      "Epoch 150/250, Loss: 0.0000\n",
      "Epoch 151/250, Loss: 0.0000\n",
      "Epoch 152/250, Loss: 0.0000\n",
      "Epoch 153/250, Loss: 0.0000\n",
      "Epoch 154/250, Loss: 0.0000\n",
      "Epoch 155/250, Loss: 0.0000\n",
      "Epoch 156/250, Loss: 0.0000\n",
      "Epoch 157/250, Loss: 0.0000\n",
      "Epoch 158/250, Loss: 0.0000\n",
      "Epoch 159/250, Loss: 0.0000\n",
      "Epoch 160/250, Loss: 0.0000\n",
      "Epoch 161/250, Loss: 0.0000\n",
      "Epoch 162/250, Loss: 0.0000\n",
      "Epoch 163/250, Loss: 0.0000\n",
      "Epoch 164/250, Loss: 0.0000\n",
      "Epoch 165/250, Loss: 0.0000\n",
      "Epoch 166/250, Loss: 0.0000\n",
      "Epoch 167/250, Loss: 0.0000\n",
      "Epoch 168/250, Loss: 0.0000\n",
      "Epoch 169/250, Loss: 0.0000\n",
      "Epoch 170/250, Loss: 0.0000\n",
      "Epoch 171/250, Loss: 0.0000\n",
      "Epoch 172/250, Loss: 0.0000\n",
      "Epoch 173/250, Loss: 0.0000\n",
      "Epoch 174/250, Loss: 0.0000\n",
      "Epoch 175/250, Loss: 0.0000\n",
      "Epoch 176/250, Loss: 0.0000\n",
      "Epoch 177/250, Loss: 0.0000\n",
      "Epoch 178/250, Loss: 0.0000\n",
      "Epoch 179/250, Loss: 0.0000\n",
      "Epoch 180/250, Loss: 0.0000\n",
      "Epoch 181/250, Loss: 0.0000\n",
      "Epoch 182/250, Loss: 0.0000\n",
      "Epoch 183/250, Loss: 0.0000\n",
      "Epoch 184/250, Loss: 0.0000\n",
      "Epoch 185/250, Loss: 0.0000\n",
      "Epoch 186/250, Loss: 0.0000\n",
      "Epoch 187/250, Loss: 0.0000\n",
      "Epoch 188/250, Loss: 0.0000\n",
      "Epoch 189/250, Loss: 0.0000\n",
      "Epoch 190/250, Loss: 0.0000\n",
      "Epoch 191/250, Loss: 0.0000\n",
      "Epoch 192/250, Loss: 0.0000\n",
      "Epoch 193/250, Loss: 0.0000\n",
      "Epoch 194/250, Loss: 0.0000\n",
      "Epoch 195/250, Loss: 0.0000\n",
      "Epoch 196/250, Loss: 0.0000\n",
      "Epoch 197/250, Loss: 0.0000\n",
      "Epoch 198/250, Loss: 0.0000\n",
      "Epoch 199/250, Loss: 0.0000\n",
      "Epoch 200/250, Loss: 0.0000\n",
      "Epoch 201/250, Loss: 0.0000\n",
      "Epoch 202/250, Loss: 0.0000\n",
      "Epoch 203/250, Loss: 0.0000\n",
      "Epoch 204/250, Loss: 0.0000\n",
      "Epoch 205/250, Loss: 0.0000\n",
      "Epoch 206/250, Loss: 0.0000\n",
      "Epoch 207/250, Loss: 0.0000\n",
      "Epoch 208/250, Loss: 0.0000\n",
      "Epoch 209/250, Loss: 0.0000\n",
      "Epoch 210/250, Loss: 0.0000\n",
      "Epoch 211/250, Loss: 0.0000\n",
      "Epoch 212/250, Loss: 0.0000\n",
      "Epoch 213/250, Loss: 0.0000\n",
      "Epoch 214/250, Loss: 0.0000\n",
      "Epoch 215/250, Loss: 0.0000\n",
      "Epoch 216/250, Loss: 0.0000\n",
      "Epoch 217/250, Loss: 0.0000\n",
      "Epoch 218/250, Loss: 0.0000\n",
      "Epoch 219/250, Loss: 0.0000\n",
      "Epoch 220/250, Loss: 0.0000\n",
      "Epoch 221/250, Loss: 0.0000\n",
      "Epoch 222/250, Loss: 0.0000\n",
      "Epoch 223/250, Loss: 0.0000\n",
      "Epoch 224/250, Loss: 0.0000\n",
      "Epoch 225/250, Loss: 0.0000\n",
      "Epoch 226/250, Loss: 0.0000\n",
      "Epoch 227/250, Loss: 0.0000\n",
      "Epoch 228/250, Loss: 0.0000\n",
      "Epoch 229/250, Loss: 0.0000\n",
      "Epoch 230/250, Loss: 0.0000\n",
      "Epoch 231/250, Loss: 0.0000\n",
      "Epoch 232/250, Loss: 0.0000\n",
      "Epoch 233/250, Loss: 0.0000\n",
      "Epoch 234/250, Loss: 0.0000\n",
      "Epoch 235/250, Loss: 0.0000\n",
      "Epoch 236/250, Loss: 0.0000\n",
      "Epoch 237/250, Loss: 0.0000\n",
      "Epoch 238/250, Loss: 0.0000\n",
      "Epoch 239/250, Loss: 0.0000\n",
      "Epoch 240/250, Loss: 0.0000\n",
      "Epoch 241/250, Loss: 0.0000\n",
      "Epoch 242/250, Loss: 0.0000\n",
      "Epoch 243/250, Loss: 0.0000\n",
      "Epoch 244/250, Loss: 0.0000\n",
      "Epoch 245/250, Loss: 0.0000\n",
      "Epoch 246/250, Loss: 0.0000\n",
      "Epoch 247/250, Loss: 0.0000\n",
      "Epoch 248/250, Loss: 0.0000\n",
      "Epoch 249/250, Loss: 0.0000\n",
      "Epoch 250/250, Loss: 0.0000\n",
      "Evaluating MLP with ReLU activation for 250 epochs...\n",
      "Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.7902439024390244,\n",
       " 10: 0.8682926829268293,\n",
       " 25: 0.9512195121951219,\n",
       " 50: 1.0,\n",
       " 100: 1.0,\n",
       " 250: 1.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine the MLP model with dynamic activation functions\n",
    "class MLPModelWithActivation(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, activation_fn):\n",
    "        super(MLPModelWithActivation, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for neurons in hidden_layers:\n",
    "            layers.append(nn.Linear(current_size, neurons))\n",
    "            layers.append(activation_fn())  # Use the dynamic activation function\n",
    "            current_size = neurons\n",
    "        layers.append(nn.Linear(current_size, 2))  # Output layer (binary classification)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Redefine the training and evaluation functions\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Experiment with different epoch values\n",
    "epoch_values = [1, 10, 25, 50, 100, 250]\n",
    "epoch_results = {}\n",
    "\n",
    "# Train and evaluate for each epoch value\n",
    "for epochs in epoch_values:\n",
    "    print(f\"\\nTraining MLP with ReLU activation for {epochs} epochs...\")\n",
    "    model = MLPModelWithActivation(input_size, [16, 32, 64], nn.ReLU)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, nn.CrossEntropyLoss(), optimizer, epochs=epochs)\n",
    "    print(f\"Evaluating MLP with ReLU activation for {epochs} epochs...\")\n",
    "    \n",
    "    # Evaluate and store the accuracy\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    epoch_results[epochs] = accuracy\n",
    "\n",
    "# Display the results\n",
    "epoch_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP with learning rate 10...\n",
      "Epoch 1/20, Loss: 5238958.3678\n",
      "Epoch 2/20, Loss: 43164.0615\n",
      "Epoch 3/20, Loss: 0.7745\n",
      "Epoch 4/20, Loss: 0.8194\n",
      "Epoch 5/20, Loss: 0.7642\n",
      "Epoch 6/20, Loss: 0.7280\n",
      "Epoch 7/20, Loss: 0.8356\n",
      "Epoch 8/20, Loss: 1.0800\n",
      "Epoch 9/20, Loss: 0.9173\n",
      "Epoch 10/20, Loss: 0.7979\n",
      "Epoch 11/20, Loss: 0.8378\n",
      "Epoch 12/20, Loss: 0.8428\n",
      "Epoch 13/20, Loss: 0.8440\n",
      "Epoch 14/20, Loss: 0.8834\n",
      "Epoch 15/20, Loss: 1.1487\n",
      "Epoch 16/20, Loss: 0.7685\n",
      "Epoch 17/20, Loss: 0.8320\n",
      "Epoch 18/20, Loss: 0.9086\n",
      "Epoch 19/20, Loss: 0.8524\n",
      "Epoch 20/20, Loss: 0.8273\n",
      "Evaluating MLP with learning rate 10...\n",
      "Accuracy: 0.5171\n",
      "\n",
      "Training MLP with learning rate 1...\n",
      "Epoch 1/20, Loss: 637.1342\n",
      "Epoch 2/20, Loss: 16.9691\n",
      "Epoch 3/20, Loss: 1.3581\n",
      "Epoch 4/20, Loss: 1.0291\n",
      "Epoch 5/20, Loss: 0.7654\n",
      "Epoch 6/20, Loss: 0.6814\n",
      "Epoch 7/20, Loss: 0.6639\n",
      "Epoch 8/20, Loss: 35.0437\n",
      "Epoch 9/20, Loss: 2.4257\n",
      "Epoch 10/20, Loss: 0.7069\n",
      "Epoch 11/20, Loss: 0.6973\n",
      "Epoch 12/20, Loss: 0.6932\n",
      "Epoch 13/20, Loss: 0.7388\n",
      "Epoch 14/20, Loss: 0.6608\n",
      "Epoch 15/20, Loss: 0.6480\n",
      "Epoch 16/20, Loss: 0.7427\n",
      "Epoch 17/20, Loss: 0.8093\n",
      "Epoch 18/20, Loss: 0.7485\n",
      "Epoch 19/20, Loss: 0.6470\n",
      "Epoch 20/20, Loss: 0.6950\n",
      "Evaluating MLP with learning rate 1...\n",
      "Accuracy: 0.6098\n",
      "\n",
      "Training MLP with learning rate 0.1...\n",
      "Epoch 1/20, Loss: 0.5548\n",
      "Epoch 2/20, Loss: 0.4724\n",
      "Epoch 3/20, Loss: 0.3809\n",
      "Epoch 4/20, Loss: 0.3398\n",
      "Epoch 5/20, Loss: 0.3488\n",
      "Epoch 6/20, Loss: 0.2605\n",
      "Epoch 7/20, Loss: 0.2458\n",
      "Epoch 8/20, Loss: 0.2476\n",
      "Epoch 9/20, Loss: 0.2446\n",
      "Epoch 10/20, Loss: 0.3406\n",
      "Epoch 11/20, Loss: 0.3076\n",
      "Epoch 12/20, Loss: 0.3617\n",
      "Epoch 13/20, Loss: 0.3373\n",
      "Epoch 14/20, Loss: 0.2739\n",
      "Epoch 15/20, Loss: 0.1679\n",
      "Epoch 16/20, Loss: 0.2507\n",
      "Epoch 17/20, Loss: 0.2651\n",
      "Epoch 18/20, Loss: 0.3516\n",
      "Epoch 19/20, Loss: 0.4147\n",
      "Epoch 20/20, Loss: 0.2237\n",
      "Evaluating MLP with learning rate 0.1...\n",
      "Accuracy: 0.8878\n",
      "\n",
      "Training MLP with learning rate 0.01...\n",
      "Epoch 1/20, Loss: 0.4753\n",
      "Epoch 2/20, Loss: 0.3280\n",
      "Epoch 3/20, Loss: 0.2794\n",
      "Epoch 4/20, Loss: 0.2369\n",
      "Epoch 5/20, Loss: 0.1776\n",
      "Epoch 6/20, Loss: 0.1486\n",
      "Epoch 7/20, Loss: 0.1342\n",
      "Epoch 8/20, Loss: 0.1072\n",
      "Epoch 9/20, Loss: 0.1034\n",
      "Epoch 10/20, Loss: 0.1104\n",
      "Epoch 11/20, Loss: 0.0586\n",
      "Epoch 12/20, Loss: 0.0408\n",
      "Epoch 13/20, Loss: 0.0242\n",
      "Epoch 14/20, Loss: 0.0186\n",
      "Epoch 15/20, Loss: 0.0088\n",
      "Epoch 16/20, Loss: 0.0238\n",
      "Epoch 17/20, Loss: 0.0845\n",
      "Epoch 18/20, Loss: 0.0822\n",
      "Epoch 19/20, Loss: 0.0559\n",
      "Epoch 20/20, Loss: 0.0457\n",
      "Evaluating MLP with learning rate 0.01...\n",
      "Accuracy: 0.9902\n",
      "\n",
      "Training MLP with learning rate 0.001...\n",
      "Epoch 1/20, Loss: 0.6705\n",
      "Epoch 2/20, Loss: 0.5557\n",
      "Epoch 3/20, Loss: 0.4089\n",
      "Epoch 4/20, Loss: 0.3597\n",
      "Epoch 5/20, Loss: 0.3340\n",
      "Epoch 6/20, Loss: 0.3187\n",
      "Epoch 7/20, Loss: 0.3029\n",
      "Epoch 8/20, Loss: 0.2911\n",
      "Epoch 9/20, Loss: 0.2770\n",
      "Epoch 10/20, Loss: 0.2681\n",
      "Epoch 11/20, Loss: 0.2504\n",
      "Epoch 12/20, Loss: 0.2420\n",
      "Epoch 13/20, Loss: 0.2224\n",
      "Epoch 14/20, Loss: 0.2068\n",
      "Epoch 15/20, Loss: 0.1929\n",
      "Epoch 16/20, Loss: 0.1785\n",
      "Epoch 17/20, Loss: 0.1762\n",
      "Epoch 18/20, Loss: 0.1580\n",
      "Epoch 19/20, Loss: 0.1442\n",
      "Epoch 20/20, Loss: 0.1366\n",
      "Evaluating MLP with learning rate 0.001...\n",
      "Accuracy: 0.9171\n",
      "\n",
      "Training MLP with learning rate 0.0001...\n",
      "Epoch 1/20, Loss: 0.7049\n",
      "Epoch 2/20, Loss: 0.6982\n",
      "Epoch 3/20, Loss: 0.6924\n",
      "Epoch 4/20, Loss: 0.6860\n",
      "Epoch 5/20, Loss: 0.6798\n",
      "Epoch 6/20, Loss: 0.6718\n",
      "Epoch 7/20, Loss: 0.6632\n",
      "Epoch 8/20, Loss: 0.6538\n",
      "Epoch 9/20, Loss: 0.6422\n",
      "Epoch 10/20, Loss: 0.6288\n",
      "Epoch 11/20, Loss: 0.6137\n",
      "Epoch 12/20, Loss: 0.5959\n",
      "Epoch 13/20, Loss: 0.5763\n",
      "Epoch 14/20, Loss: 0.5537\n",
      "Epoch 15/20, Loss: 0.5319\n",
      "Epoch 16/20, Loss: 0.5084\n",
      "Epoch 17/20, Loss: 0.4865\n",
      "Epoch 18/20, Loss: 0.4633\n",
      "Epoch 19/20, Loss: 0.4437\n",
      "Epoch 20/20, Loss: 0.4289\n",
      "Evaluating MLP with learning rate 0.0001...\n",
      "Accuracy: 0.8195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{10: 0.5170731707317073,\n",
       " 1: 0.6097560975609756,\n",
       " 0.1: 0.8878048780487805,\n",
       " 0.01: 0.9902439024390244,\n",
       " 0.001: 0.9170731707317074,\n",
       " 0.0001: 0.8195121951219512}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the learning rates to test\n",
    "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
    "lr_results = {}\n",
    "\n",
    "# Train and evaluate for each learning rate\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining MLP with learning rate {lr}...\")\n",
    "    model = MLPModelWithActivation(input_size, [16, 32, 64], nn.ReLU)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, nn.CrossEntropyLoss(), optimizer, epochs=20)\n",
    "    print(f\"Evaluating MLP with learning rate {lr}...\")\n",
    "    \n",
    "    # Evaluate and store the accuracy\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    lr_results[lr] = accuracy\n",
    "\n",
    "# Display the results\n",
    "lr_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP with batch size 16...\n",
      "Epoch 1/20, Loss: 0.4724\n",
      "Epoch 2/20, Loss: 0.3146\n",
      "Epoch 3/20, Loss: 0.2604\n",
      "Epoch 4/20, Loss: 0.2278\n",
      "Epoch 5/20, Loss: 0.1814\n",
      "Epoch 6/20, Loss: 0.1671\n",
      "Epoch 7/20, Loss: 0.1339\n",
      "Epoch 8/20, Loss: 0.0827\n",
      "Epoch 9/20, Loss: 0.1987\n",
      "Epoch 10/20, Loss: 0.0871\n",
      "Epoch 11/20, Loss: 0.0469\n",
      "Epoch 12/20, Loss: 0.1111\n",
      "Epoch 13/20, Loss: 0.0918\n",
      "Epoch 14/20, Loss: 0.0455\n",
      "Epoch 15/20, Loss: 0.0515\n",
      "Epoch 16/20, Loss: 0.0630\n",
      "Epoch 17/20, Loss: 0.0498\n",
      "Epoch 18/20, Loss: 0.0538\n",
      "Epoch 19/20, Loss: 0.0387\n",
      "Epoch 20/20, Loss: 0.0341\n",
      "Evaluating MLP with batch size 16...\n",
      "Accuracy: 0.9756\n",
      "\n",
      "Training MLP with batch size 32...\n",
      "Epoch 1/20, Loss: 0.4633\n",
      "Epoch 2/20, Loss: 0.3346\n",
      "Epoch 3/20, Loss: 0.2800\n",
      "Epoch 4/20, Loss: 0.2233\n",
      "Epoch 5/20, Loss: 0.2104\n",
      "Epoch 6/20, Loss: 0.1895\n",
      "Epoch 7/20, Loss: 0.1376\n",
      "Epoch 8/20, Loss: 0.0849\n",
      "Epoch 9/20, Loss: 0.1118\n",
      "Epoch 10/20, Loss: 0.1382\n",
      "Epoch 11/20, Loss: 0.0645\n",
      "Epoch 12/20, Loss: 0.0431\n",
      "Epoch 13/20, Loss: 0.0296\n",
      "Epoch 14/20, Loss: 0.0170\n",
      "Epoch 15/20, Loss: 0.0063\n",
      "Epoch 16/20, Loss: 0.0021\n",
      "Epoch 17/20, Loss: 0.0009\n",
      "Epoch 18/20, Loss: 0.0006\n",
      "Epoch 19/20, Loss: 0.0005\n",
      "Epoch 20/20, Loss: 0.0004\n",
      "Evaluating MLP with batch size 32...\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Training MLP with batch size 64...\n",
      "Epoch 1/20, Loss: 0.5153\n",
      "Epoch 2/20, Loss: 0.3702\n",
      "Epoch 3/20, Loss: 0.3174\n",
      "Epoch 4/20, Loss: 0.2731\n",
      "Epoch 5/20, Loss: 0.2399\n",
      "Epoch 6/20, Loss: 0.1994\n",
      "Epoch 7/20, Loss: 0.1712\n",
      "Epoch 8/20, Loss: 0.1612\n",
      "Epoch 9/20, Loss: 0.1198\n",
      "Epoch 10/20, Loss: 0.0979\n",
      "Epoch 11/20, Loss: 0.0992\n",
      "Epoch 12/20, Loss: 0.0893\n",
      "Epoch 13/20, Loss: 0.0634\n",
      "Epoch 14/20, Loss: 0.0378\n",
      "Epoch 15/20, Loss: 0.0517\n",
      "Epoch 16/20, Loss: 0.0186\n",
      "Epoch 17/20, Loss: 0.0108\n",
      "Epoch 18/20, Loss: 0.0065\n",
      "Epoch 19/20, Loss: 0.0040\n",
      "Epoch 20/20, Loss: 0.0028\n",
      "Evaluating MLP with batch size 64...\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Training MLP with batch size 128...\n",
      "Epoch 1/20, Loss: 0.5992\n",
      "Epoch 2/20, Loss: 0.3934\n",
      "Epoch 3/20, Loss: 0.3288\n",
      "Epoch 4/20, Loss: 0.2919\n",
      "Epoch 5/20, Loss: 0.2453\n",
      "Epoch 6/20, Loss: 0.2294\n",
      "Epoch 7/20, Loss: 0.2217\n",
      "Epoch 8/20, Loss: 0.1744\n",
      "Epoch 9/20, Loss: 0.1728\n",
      "Epoch 10/20, Loss: 0.1473\n",
      "Epoch 11/20, Loss: 0.0993\n",
      "Epoch 12/20, Loss: 0.0896\n",
      "Epoch 13/20, Loss: 0.0762\n",
      "Epoch 14/20, Loss: 0.0792\n",
      "Epoch 15/20, Loss: 0.0478\n",
      "Epoch 16/20, Loss: 0.0634\n",
      "Epoch 17/20, Loss: 0.0445\n",
      "Epoch 18/20, Loss: 0.0241\n",
      "Epoch 19/20, Loss: 0.0159\n",
      "Epoch 20/20, Loss: 0.0094\n",
      "Evaluating MLP with batch size 128...\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Training MLP with batch size 256...\n",
      "Epoch 1/20, Loss: 0.6639\n",
      "Epoch 2/20, Loss: 0.5532\n",
      "Epoch 3/20, Loss: 0.4410\n",
      "Epoch 4/20, Loss: 0.3650\n",
      "Epoch 5/20, Loss: 0.3353\n",
      "Epoch 6/20, Loss: 0.3151\n",
      "Epoch 7/20, Loss: 0.2775\n",
      "Epoch 8/20, Loss: 0.2723\n",
      "Epoch 9/20, Loss: 0.2620\n",
      "Epoch 10/20, Loss: 0.2355\n",
      "Epoch 11/20, Loss: 0.2261\n",
      "Epoch 12/20, Loss: 0.1823\n",
      "Epoch 13/20, Loss: 0.1570\n",
      "Epoch 14/20, Loss: 0.1530\n",
      "Epoch 15/20, Loss: 0.1339\n",
      "Epoch 16/20, Loss: 0.1151\n",
      "Epoch 17/20, Loss: 0.0975\n",
      "Epoch 18/20, Loss: 0.0833\n",
      "Epoch 19/20, Loss: 0.0810\n",
      "Epoch 20/20, Loss: 0.0607\n",
      "Evaluating MLP with batch size 256...\n",
      "Accuracy: 0.9659\n",
      "\n",
      "Training MLP with batch size 512...\n",
      "Epoch 1/20, Loss: 0.6723\n",
      "Epoch 2/20, Loss: 0.6061\n",
      "Epoch 3/20, Loss: 0.4981\n",
      "Epoch 4/20, Loss: 0.4236\n",
      "Epoch 5/20, Loss: 0.3994\n",
      "Epoch 6/20, Loss: 0.3854\n",
      "Epoch 7/20, Loss: 0.3726\n",
      "Epoch 8/20, Loss: 0.3457\n",
      "Epoch 9/20, Loss: 0.3315\n",
      "Epoch 10/20, Loss: 0.3211\n",
      "Epoch 11/20, Loss: 0.3048\n",
      "Epoch 12/20, Loss: 0.2871\n",
      "Epoch 13/20, Loss: 0.2914\n",
      "Epoch 14/20, Loss: 0.2750\n",
      "Epoch 15/20, Loss: 0.2638\n",
      "Epoch 16/20, Loss: 0.2569\n",
      "Epoch 17/20, Loss: 0.2582\n",
      "Epoch 18/20, Loss: 0.2305\n",
      "Epoch 19/20, Loss: 0.2227\n",
      "Epoch 20/20, Loss: 0.2105\n",
      "Evaluating MLP with batch size 512...\n",
      "Accuracy: 0.9171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{16: 0.975609756097561,\n",
       " 32: 1.0,\n",
       " 64: 1.0,\n",
       " 128: 1.0,\n",
       " 256: 0.9658536585365853,\n",
       " 512: 0.9170731707317074}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define batch sizes to test\n",
    "batch_sizes = [16, 32, 64, 128, 256, 512]\n",
    "batch_results = {}\n",
    "\n",
    "# Train and evaluate for each batch size\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nTraining MLP with batch size {batch_size}...\")\n",
    "    \n",
    "    # Create new DataLoader with the current batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Define the model, optimizer, and loss function\n",
    "    model = MLPModelWithActivation(input_size, [16, 32, 64], nn.ReLU)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, nn.CrossEntropyLoss(), optimizer, epochs=20)\n",
    "    print(f\"Evaluating MLP with batch size {batch_size}...\")\n",
    "    \n",
    "    # Evaluate and store the accuracy\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    batch_results[batch_size] = accuracy\n",
    "\n",
    "# Display the results\n",
    "batch_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
